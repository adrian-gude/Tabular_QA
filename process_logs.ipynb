{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Union\n",
    "from tqdm import tqdm\n",
    "from databench_eval import Evaluator, utils\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEvaluator(Evaluator):\n",
    "    def eval(\n",
    "        self,\n",
    "        responses: Union[List[str], str],\n",
    "        lite: bool = False,\n",
    "    ) -> dict:\n",
    "        if isinstance(responses, str):\n",
    "            with open(responses, \"r\") as f:\n",
    "                responses = f.read().splitlines()\n",
    "\n",
    "        correct = {\n",
    "            \"average\": 0,\n",
    "            \"category\": 0,\n",
    "            \"boolean\": 0,\n",
    "            \"number\": 0,\n",
    "            \"list[category]\": 0,\n",
    "            \"list[number]\": 0,\n",
    "        }\n",
    "        truths = self.qa[\"answer\"] if not lite else self.qa[\"sample_answer\"]\n",
    "        evals = {\n",
    "            \"average\": [],\n",
    "            \"category\": [],\n",
    "            \"boolean\": [],\n",
    "            \"number\": [],\n",
    "            \"list[category]\": [],\n",
    "            \"list[number]\": [],\n",
    "        }\n",
    "        for response, truth, semantic in tqdm(\n",
    "            zip(responses, truths, self.qa[\"type\"]), total=len(truths)\n",
    "        ):\n",
    "            truthy = self.compare(response, truth, semantic)\n",
    "            if self.compare(response, truth, semantic):\n",
    "                correct[\"average\"] += 1\n",
    "                correct[semantic] += 1\n",
    "            evals[\"average\"].append(truthy)\n",
    "            evals[semantic].append(truthy)\n",
    "        self.evals = evals\n",
    "        return {\n",
    "            \"average\": correct[\"average\"] / len(truths),\n",
    "            \"category\": correct[\"category\"] / len(self.evals[\"category\"]),\n",
    "            \"boolean\": correct[\"boolean\"] / len(self.evals[\"boolean\"]),\n",
    "            \"number\": correct[\"number\"] / len(self.evals[\"number\"]),\n",
    "            \"list[category]\": correct[\"list[category]\"]\n",
    "            / len(self.evals[\"list[category]\"]),\n",
    "            \"list[number]\": correct[\"list[number]\"] / len(self.evals[\"list[number]\"]),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = utils.load_qa(name=\"semeval\", split=\"dev\")\n",
    "evaluator = CustomEvaluator(qa=qa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:00<00:00, 18012.90it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 21767.04it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 20554.96it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 26300.70it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 9692.00it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 8789.87it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 19613.87it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 22329.80it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 53233.54it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 12280.88it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 24544.69it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 20729.57it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 22941.24it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 14800.76it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 11541.44it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 21133.99it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 17339.22it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 21752.22it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 20338.18it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 22769.99it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 23318.29it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 31428.31it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 15524.04it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 22848.68it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 15962.91it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 9047.92it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 20007.71it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 27507.58it/s]\n",
      "100%|██████████| 320/320 [00:00<00:00, 9426.46it/s]\n"
     ]
    }
   ],
   "source": [
    "input_file = \"logs/\"\n",
    "\n",
    "logs = []\n",
    "for filename in os.listdir(input_file):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "    with open(os.path.join(input_file, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.read().split(\"--------------------\")\n",
    "        model = lines[0].split(\"\\n\")[0].replace(\"Model:\", \"\")\n",
    "        for i in range(0, len(lines) - 1):\n",
    "            lines[i] = lines[i].split(\"Response:\")[1].strip()\n",
    "        accuracy = evaluator.eval(lines, lite=filename.endswith(\"lite.txt\"))\n",
    "        accuracy[\"log\"] = filename.replace(\".txt\", \"\")\n",
    "        accuracy[\"task\"] = \"Task 2\" if filename.endswith(\"lite.txt\") else \"Task 1\"\n",
    "        accuracy[\"model\"] = model\n",
    "        logs.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(logs)\n",
    "df = df[\n",
    "    [\n",
    "        \"log\",\n",
    "        \"model\",\n",
    "        \"task\",\n",
    "        \"average\",\n",
    "        \"boolean\",\n",
    "        \"category\",\n",
    "        \"number\",\n",
    "        \"list[category]\",\n",
    "        \"list[number]\",\n",
    "    ]\n",
    "]\n",
    "df.sort_values(by=[\"log\"])\n",
    "df.to_csv(\"evaluations.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
